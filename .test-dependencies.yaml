# SPDX-FileCopyrightText: (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# .test-dependencies.yaml
# This YAML file defines the dependencies for the test bootstrap step. It specifies build steps for various dependencies
# required for the test environment. The file contains the following fields:
#
# Fields:
# - kind-cluster-config: Specifies the configuration file for the kind cluster.
#
# - components: A list of components, each with its own configuration:
#   - name: The name of the component.
#   - skip-component: A flag to skip the component during the build process (true/false).
#   - skip-local-build: A flag to skip the local build of the component (true/false).
#   - pre-install-commands: Commands to run before installing the component.
#   - helm-repo: Details for the Helm repositories, including:
#       - url: The URL of the Helm repository.
#         release-name: The release name for the Helm chart.
#         package: The Helm chart package name.
#         namespace: The Kubernetes namespace for the Helm release.
#         version: The version of the Helm chart.
#         use-devel: A flag to enable (or not) usage of developer versions of the chart
#         overrides: The Helm chart overrides.
#   - git-repo:
#       url: The Git URL of the component's repository.
#       version: The Git branch/tag/commit of the component to use.
#   - make-directory: The directory containing the Makefile.
#   - make-variables: Variables to pass to the `make` command.
#   - make-targets: `make` targets to build the component.
#   - post-install-commands: Commands to run after installing the component.
---
kind-cluster-config: configs/kind-cluster-with-extramounts.yaml

components:
  # Cluster API Provider Intel
  - name: cluster-api-provider-intel
    skip-component: false
    skip-local-build: true
    pre-install-commands:
      - echo "Installing Prometheus Operator CRDs to get Service Monitor CRD"
      - kubectl apply -f https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.81.0/stripped-down-crds.yaml
    helm-repo:
      - url: "oci://registry-rs.edgeorchestration.intel.com"
        release-name: "intel-infra-provider"
        package: "edge-orch/cluster/charts/intel-infra-provider"
        namespace: "default"
        version: ""  # Use the latest version when nil
        use-devel: false  # Use development version of the chart
        overrides: "--set metrics.serviceMonitor.enabled=false --set manager.extraArgs.use-inv-stub=true --set southboundApi.extraArgs.useGrpcStubMiddleware=true"
      - url: "oci://registry-rs.edgeorchestration.intel.com"
        release-name: "intel-infra-provider-crds"
        package: "edge-orch/cluster/charts/intel-infra-provider-crds"
        namespace: "default"
        version: ""  # Use the latest version when nil
        use-devel: false  # Use development version of the chart
        overrides: ""
    git-repo:
      url: https://github.com/open-edge-platform/cluster-api-provider-intel.git
      version: main
    make-directory: ""
    make-variables:
      - VERSION=v0.0.0
      - HELM_VERSION=v0.0.0
      - USE_GRPC_MIDDLEWARE_STUB=true  # Enable this flag to use the gRPC middleware stub. Skips jwt auth on SB-API
      - USE_INV_STUB=true  # Enable this flag to use the Inventory stub. Inventory stub is used when we are not installing Inventory
    make-targets:
      - kind-load
      - helm-install
    post-install-commands:
      # ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #
      # Install the CAPI operator and default control plane and infra providers -- start
      # ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #
      # Install cert-manager
      - kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.16.0/cert-manager.yaml
      # Wait for cert-manager to be ready
      - kubectl wait --for=condition=Available --timeout=300s deployment.apps/cert-manager-webhook -n cert-manager
      - kubectl wait --for=condition=Available --timeout=300s deployment.apps/cert-manager-cainjector -n cert-manager
      - kubectl wait --for=condition=Available --timeout=300s deployment.apps/cert-manager -n cert-manager
      # Install the CAPI operator and default control plane and infra providers
      - helm repo add capi-operator https://kubernetes-sigs.github.io/cluster-api-operator
      - helm repo add jetstack https://charts.jetstack.io
      - helm repo update
      - kubectl apply -f ../../configs/capi-variables.yaml --force
      # Use envsubst to substitute the environment variables in the YAML file
      - envsubst < ../../configs/capi-operator.yaml > /tmp/capi-operator.yaml
      - helm install capi-operator capi-operator/cluster-api-operator --create-namespace -n capi-operator-system -f /tmp/capi-operator.yaml --wait --timeout 5m --version ${CAPI_OPERATOR_HELM_VERSION}
      # Wait for the CAPI operator to be ready
      - until kubectl get -n capi-operator-system deployment/capi-operator-cluster-api-operator >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace capi-operator-system deployment/capi-operator-cluster-api-operator --for=condition=available --timeout=5m
      - until kubectl get -n capi-system deployment/capi-controller-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace capi-system deployment/capi-controller-manager --for=condition=available --timeout=5m
      # Install the default control plane and infra providers
      - until kubectl get -n docker-infrastructure-system deployment/capd-controller-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace docker-infrastructure-system deployment/capd-controller-manager --for=condition=available --timeout=5m
      - until kubectl get -n kubeadm-bootstrap-system deployment/capi-kubeadm-bootstrap-controller-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace kubeadm-bootstrap-system deployment/capi-kubeadm-bootstrap-controller-manager --for=condition=available --timeout=5m
      - until kubectl get -n kubeadm-control-plane-system deployment/capi-kubeadm-control-plane-controller-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace kubeadm-control-plane-system deployment/capi-kubeadm-control-plane-controller-manager --for=condition=available --timeout=5m
      - until kubectl get -n capr-system deployment/rke2-bootstrap-controller-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace capr-system deployment/rke2-bootstrap-controller-manager --for=condition=available --timeout=5m
      - until kubectl get -n capr-system deployment/rke2-control-plane-controller-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace capr-system deployment/rke2-control-plane-controller-manager --for=condition=available --timeout=5m
      # Patch the K3S Providers so that they can be initialized and wait for the components to be ready
      # Note: The patch is required to set the fetchConfig URL for the K3S providers. CAPI ProviderURL for K3s seems not been supported like for  RKE2 and Kubeadm. The field providerURL, however, still is needed.
      - until kubectl get -n capk-system controlplaneprovider/k3s >/dev/null 2>&1; do sleep 1; done && kubectl patch -n capk-system controlplaneprovider k3s --type=merge -p '{"spec":{"fetchConfig":{"url":"'"${CAPI_K3S_CONTROLPLANE_URL}"'"}}}'
      - until kubectl get -n capk-system bootstrapprovider/k3s >/dev/null 2>&1; do sleep 1; done && kubectl patch -n capk-system bootstrapprovider k3s --type=merge -p '{"spec":{"fetchConfig":{"url":"'"${CAPI_K3S_BOOTSTRAP_URL}"'"}}}'
      - until kubectl get -n capk-system deployment/capi-k3s-bootstrap-controller-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace capk-system deployment/capi-k3s-bootstrap-controller-manager --for=condition=available --timeout=5m
      - until kubectl get -n capk-system deployment/capi-k3s-control-plane-controller-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --namespace capk-system deployment/capi-k3s-control-plane-controller-manager --for=condition=available --timeout=5m
      # ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #
      # Install the CAPI operator and default control plane and infra providers -- end
      # ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #
      # Wait for Intel Infrastructure Provider to be ready and patch with SHARED_SECRET_KEY
      - until kubectl get deployment intel-infra-provider-southbound >/dev/null 2>&1; do sleep 1; done && kubectl wait --for=condition=Available --timeout=300s deployment/intel-infra-provider-southbound
      - until kubectl get deployment intel-infra-provider-manager >/dev/null 2>&1; do sleep 1; done && kubectl wait --for=condition=Available --timeout=300s deployment/intel-infra-provider-manager
      - kubectl patch deployment intel-infra-provider-southbound -p '{"spec":{"template":{"spec":{"containers":[{"name":"intel-infra-provider-southbound","env":[{"name":"SHARED_SECRET_KEY","value":"randomSecretKey"}]}]}}}}'
      - kubectl patch deployment intel-infra-provider-manager -p '{"spec":{"template":{"spec":{"containers":[{"name":"intel-infra-provider-manager","env":[{"name":"SHARED_SECRET_KEY","value":"randomSecretKey"}]}]}}}}'
      - kubectl patch deployment intel-infra-provider-southbound -p '{"spec":{"template":{"spec":{"containers":[{"name":"intel-infra-provider-southbound","env":[{"name":"ALLOW_MISSING_AUTH_CLIENTS","value":"cluster-agent"}]}]}}}}'
      - kubectl patch deployment intel-infra-provider-manager -p '{"spec":{"template":{"spec":{"containers":[{"name":"intel-infra-provider-manager","env":[{"name":"ALLOW_MISSING_AUTH_CLIENTS","value":"cluster-agent"}]}]}}}}'
      - kubectl rollout status deployment/intel-infra-provider-southbound
      - kubectl rollout status deployment/intel-infra-provider-manager
      - kubectl delete -f config/crd/deps/cluster.edge-orchestrator.intel.com_clusterconnects.yaml || true

  # Cluster Connect Gateway
  - name: cluster-connect-gateway
    skip-component: false
    skip-local-build: true
    pre-install-commands: []
    helm-repo:
      - url: "oci://registry-rs.edgeorchestration.intel.com"
        release-name: "cluster-connect-gateway"
        package: "edge-orch/cluster/charts/cluster-connect-gateway"
        namespace: "default"
        version: ""  # Use the latest version when nil
        use-devel: false  # Use development version of the chart
        overrides: "--set controller.privateCA.enabled=false"
      - url: "oci://registry-rs.edgeorchestration.intel.com"
        release-name: "cluster-connect-gateway-crd"
        package: "edge-orch/cluster/charts/cluster-connect-gateway-crd"
        namespace: "default"
        version: ""  # Use the latest version when nil
        use-devel: false  # Use development version of the chart
        overrides: ""
    git-repo:
      url: https://github.com/open-edge-platform/cluster-connect-gateway.git
      version: main
    make-directory: ""
    make-variables:
      - VERSION=v0.0.0
      - HELM_VERSION=v0.0.0
      - KIND_CLUSTER=kind
      - NAMESPACE=default
      - HELM_VARS="--set controller.privateCA.enabled=false --set agent.image.tag=latest --set controller.connectionProbeTimeout=1m --set gateway.connectionProbeInterval=20s"
    make-targets:
      - docker-build
      - docker-load
      - helm-install
    post-install-commands:
      - CONNECT_GATEWAY_IP=$(kubectl get svc cluster-connect-gateway -o go-template="{{ .spec.clusterIP }}") envsubst < ../../configs/conredns-config.yaml | kubectl apply -f -

  # Cluster Manager
  - name: cluster-manager
    skip-component: false
    skip-local-build: true
    pre-install-commands: []
    helm-repo:
      - url: "oci://registry-rs.edgeorchestration.intel.com"
        release-name: "cluster-manager"
        package: "edge-orch/cluster/charts/cluster-manager"
        namespace: "default"
        version: ""  # Use the latest version when nil
        use-devel: false  # Use development version of the chart
        overrides: "--set clusterManager.extraArgs.disable-mt=true --set clusterManager.extraArgs.disable-auth=true --set clusterManager.extraArgs.disable-inventory=true --set templateController.extraArgs[0]='--webhook-enabled=true' --set webhookService.enabled=true"
      - url: "oci://registry-rs.edgeorchestration.intel.com"
        release-name: "cluster-template-crd"
        package: "edge-orch/cluster/charts/cluster-template-crd"
        namespace: "default"
        version: ""  # Use the latest version when nil
        use-devel: false  # Use development version of the chart
        overrides: ""
    git-repo:
      url: https://github.com/open-edge-platform/cluster-manager.git
      version: main
    make-variables:
      - VERSION=v0.0.0
      - HELM_VERSION=v0.0.0
      - KIND_CLUSTER=kind
      - DISABLE_MT=true  # Enable this flag to disable the multi-tenancy feature. This is required for the test environment where no MT controllers are installed
      - DISABLE_AUTH=true  # Should be true for CO subsystem integration tests if keycloak is not deployed
      - DISABLE_INV=true  # Should be true for CO subsystem integration tests if inventory is not deployed
    make-targets:
      - helm-install
    make-directory: ""
    post-install-commands: []

  # Cluster Agent on LW-ENiC
  - name: cluster-agent
    skip-component: false
    skip-local-build: true  # We need 'false' to build locally if no helm chart exists
    pre-install-commands: []
    helm-repo: []
    git-repo:
      url: https://github.com/open-edge-platform/virtual-edge-node.git
      version: main
    make-directory: "edge-node-container/"
    make-variables:
      - SHELL=/bin/bash
      - PATH=/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    make-targets:
      - bma_packages         # Download BMA packages
      - docker-build-enic    # Build ENiC Docker image locally
    post-install-commands:
      # Load ENiC image into kind cluster and deploy
      - kind load docker-image 080137407410.dkr.ecr.us-west-2.amazonaws.com/edge-orch/infra/enic:0.8.5 --name kind
      # Setup comprehensive authentication bypass infrastructure for CI
      - |
        # Wait for pod to be ready
        kubectl wait --for=condition=Ready pod/cluster-agent-0 --timeout=300s || true
        sleep 10  # Give container time to fully initialize

        # Setup CA certificate for HTTPS trust chain
        kubectl exec cluster-agent-0 -- bash -c "
        mkdir -p /usr/local/share/ca-certificates
        openssl req -x509 -newkey rsa:2048 -keyout /tmp/key.pem -out /tmp/cert.pem -days 365 -nodes -subj '/CN=keycloak.localhost'
        cp /tmp/cert.pem /usr/local/share/ca-certificates/ca.crt
        update-ca-certificates
        "

        # Setup DNS resolution for keycloak.localhost
        kubectl exec cluster-agent-0 -- bash -c "
        echo '127.0.0.1 keycloak.localhost' >> /etc/hosts
        "

        # Create and start mock HTTPS Keycloak service
        kubectl exec cluster-agent-0 -- bash -c "
        cat > /tmp/mock_keycloak.py << 'PYEOF'
        # !/usr/bin/env python3
        import http.server
        import ssl
        import json
        import socketserver
        from urllib.parse import urlparse, parse_qs

        class MockKeycloakHandler(http.server.BaseHTTPRequestHandler):
            def do_GET(self):
                self.send_response(200)
                self.send_header('Content-type', 'text/html')
                self.end_headers()
                self.wfile.write(b'Mock Keycloak Server')

            def do_POST(self):
                if '/realms/master/protocol/openid-connect/token' in self.path:
                    self.send_response(200)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {
                        'access_token': 'fake_token_for_testing',
                        'token_type': 'Bearer',
                        'expires_in': 3600
                    }
                    self.wfile.write(json.dumps(response).encode())
                else:
                    self.send_response(404)
                    self.end_headers()

            def log_message(self, format, *args):
                pass  # Suppress log messages

        # Create HTTPS server
        httpd = socketserver.TCPServer(('', 443), MockKeycloakHandler)
        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
        context.load_cert_chain('/tmp/cert.pem', '/tmp/key.pem')
        httpd.socket = context.wrap_socket(httpd.socket, server_side=True)
        httpd.serve_forever()
        PYEOF

        # Make it executable and start as background service
        chmod +x /tmp/mock_keycloak.py
        nohup python3 /tmp/mock_keycloak.py > /tmp/mock_keycloak.log 2>&1 &
        "

        # Setup BMA agent environment variables for consistent installation
        kubectl exec cluster-agent-0 -- bash -c "
        # Set all environment variables in one session
        export _ORCH_FQDN_=localhost
        export _NODE_AGENT_VERSION_=1.7.3
        export _CLUSTER_AGENT_VERSION_=1.7.4
        export _HDA_AGENT_VERSION_=1.7.1
        export _POA_AGENT_VERSION_=1.9.0
        export _TRTL_VERSION_=4.2.8.6-1
        export _INBM_CLOUDADAPTER_AGENT_VERSION_=4.2.8.6-1
        export _INBM_DISPATCHER_AGENT_VERSION_=4.2.8.6-1
        export _INBM_CONFIGURATION_AGENT_VERSION_=4.2.8.6-1
        export _INBM_TELEMETRY_AGENT_VERSION_=4.2.8.6-1
        export _INBM_DIAGNOSTIC_AGENT_VERSION_=4.2.8.6-1
        export _INBC_PROGRAM_VERSION_=4.2.8.6-1
        export _MQTT_VERSION_=4.2.8.6-1
        export _TPM_PROVISION_VERSION_=4.2.8.6-1
        export _PLATFORM_UPDATE_AGENT_VERSION_=1.5.2
        export _PLATFORM_TELEMETRY_AGENT_VERSION_=1.4.0
        export _CADDY_VERSION_=2.7.6

        # Generate proper agents environment file with all variables in same context
        envsubst < /etc/agents_env.tpl > /opt/enic/bin/agents_env.sh

        # Create required directories and files for BMA agents
        mkdir -p /etc/intel_edge_node/client-credentials
        mkdir -p /etc/intel_edge_node/tokens/cluster-agent
        mkdir -p /etc/intel_edge_node/tokens/node-agent
        mkdir -p etc/intel_edge_node/client-credentials

        # Create mock credentials and tokens
        echo 'mock_client_secret_for_testing' > /etc/intel_edge_node/client-credentials/client_secret
        echo 'mock_access_token' > /etc/intel_edge_node/tokens/cluster-agent/access_token
        echo 'mock_access_token' > /etc/intel_edge_node/tokens/node-agent/access_token
        touch etc/intel_edge_node/client-credentials/dummy
        chmod 600 /etc/intel_edge_node/client-credentials/client_secret
        chmod 600 /etc/intel_edge_node/tokens/cluster-agent/access_token
        chmod 600 /etc/intel_edge_node/tokens/node-agent/access_token
        chmod 600 etc/intel_edge_node/client-credentials/dummy

        # Create placeholder files to avoid package installation errors
        touch /etc/intel_edge_node/client-credentials/placeholder
        chmod 600 /etc/intel_edge_node/client-credentials/placeholder

        # Fix any broken package states
        dpkg --configure -a || true

        # Stop the failing agents service temporarily
        systemctl stop agents.service || true

        # Install critical packages manually with error handling
        cd /opt/bma_packages

        # Install node-agent manually with workaround for post-install script
        if [ -f node-agent_1.7.3_amd64.deb ]; then
            echo 'node-agent node-agent/onboarding.serviceURL string infra-node.localhost:443' | debconf-set-selections
            echo 'node-agent node-agent/auth.accessTokenURL string keycloak.localhost' | debconf-set-selections
            echo 'node-agent node-agent/auth.rsTokenURL string release.localhost' | debconf-set-selections
            echo 'node-agent node-agent/auth.RSType string no-auth' | debconf-set-selections
            echo 'node-agent node-agent/proxy.aptSourceURL string files-rs.edgeorchestration.intel.com' | debconf-set-selections
            echo 'node-agent node-agent/proxy.aptSourceProxyPort string 60444' | debconf-set-selections
            echo 'node-agent node-agent/proxy.aptSourceFilesRSRoot string files-edge-orch' | debconf-set-selections

            # Extract and install package bypassing post-install script issues
            dpkg --unpack node-agent_1.7.3_amd64.deb || true
            # Fix the post-install script issue by ensuring files exist
            mkdir -p etc/intel_edge_node/client-credentials 2>/dev/null || true
            touch etc/intel_edge_node/client-credentials/dummy
            dpkg --configure node-agent || true
        fi

        # Install cluster-agent manually
        if [ -f cluster-agent_1.7.4_amd64.deb ]; then
            echo 'cluster-agent cluster-agent/cluster-orchestrator-url string cluster-orch-node.localhost:443' | debconf-set-selections
            apt-get install -y -o Dpkg::Options::='--force-confnew' ./cluster-agent_1.7.4_amd64.deb || true
        fi

        # Update cluster-agent configuration with correct UUID
        if [ -f /etc/edge-node/node/confs/cluster-agent.yaml ]; then
            sed -i \"s/GUID: '[^']*'/GUID: '12345678-1234-1234-1234-123456789012'/\" /etc/edge-node/node/confs/cluster-agent.yaml
        fi

        # Update node-agent configuration with correct UUID
        if [ -f /etc/edge-node/node/confs/node-agent.yaml ]; then
            sed -i \"s/GUID: '[^']*'/GUID: '12345678-1234-1234-1234-123456789012'/\" /etc/edge-node/node/confs/node-agent.yaml
        fi

        # FIX THE CRITICAL sed SYNTAX ERRORS IN agents.sh SCRIPT TO PREVENT SERVICE CYCLING
        # This is the core fix for stable agents.service operation
        if [ -f /opt/enic/bin/agents.sh ]; then
            # Create backup
            cp /opt/enic/bin/agents.sh /opt/enic/bin/agents.sh.backup

            # CRITICAL FIX: Fix ALL package naming issues first (must be done before service starts)
            sed -i 's|caddy_\${CADDY_VERSION}_linux_amd64.deb|caddy_2.7.6_linux_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|node-agent_\${NODE_AGENT_VERSION}_amd64.deb|node-agent_1.7.3_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|cluster-agent_\${CLUSTER_AGENT_VERSION}_amd64.deb|cluster-agent_1.7.4_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|hardware-discovery-agent_\${HDA_AGENT_VERSION}_amd64.deb|hardware-discovery-agent_1.7.1_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|platform-observability-agent_\${POA_AGENT_VERSION}_amd64.deb|platform-observability-agent_1.9.0_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|platform-update-agent_\${PLATFORM_UPDATE_AGENT_VERSION}_amd64.deb|platform-update-agent_1.5.2_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|platform-telemetry-agent_\${PLATFORM_TELEMETRY_AGENT_VERSION}_amd64.deb|platform-telemetry-agent_1.4.0_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|trtl-\${TRTL_VERSION}.EVAL.deb|trtl-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-cloudadapter-agent-\${INBM_CLOUDADAPTER_AGENT_VERSION}.EVAL.deb|inbm-cloudadapter-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-dispatcher-agent-\${INBM_DISPATCHER_AGENT_VERSION}.EVAL.deb|inbm-dispatcher-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-configuration-agent-\${INBM_CONFIGURATION_AGENT_VERSION}.EVAL.deb|inbm-configuration-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-telemetry-agent-\${INBM_TELEMETRY_AGENT_VERSION}.EVAL.deb|inbm-telemetry-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-diagnostic-agent-\${INBM_DIAGNOSTIC_AGENT_VERSION}.EVAL.deb|inbm-diagnostic-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbc-program-\${INBC_PROGRAM_VERSION}.EVAL.deb|inbc-program-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|mqtt-\${MQTT_VERSION}.deb|mqtt-4.2.8.6-1.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|tpm-provision-\${TPM_PROVISION_VERSION}.deb|tpm-provision-4.2.8.6-1.deb|g' /opt/enic/bin/agents.sh

            # Fix line 293 (platform-update-agent) - remove problematic variable sed and use hardcoded UUID
            sed -i '293s/.*/    sed -i \"s|GUID: '\''.*'\''|GUID: '\''12345678-1234-1234-1234-123456789012'\''|\" \/etc\/edge-node\/node\/confs\/platform-update-agent.yaml/' /opt/enic/bin/agents.sh

            # Fix line 323 (cluster-agent) - remove problematic variable sed and use hardcoded UUID
            sed -i '323s/.*/    sed -i \"s|GUID: '\''.*'\''|GUID: '\''12345678-1234-1234-1234-123456789012'\''|\" \/etc\/edge-node\/node\/confs\/cluster-agent.yaml/' /opt/enic/bin/agents.sh

            # Fix line 335 (node-agent) - remove problematic variable sed and use hardcoded UUID
            sed -i '335s/.*/    sed -i \"s|GUID: '\''.*'\''|GUID: '\''12345678-1234-1234-1234-123456789012'\''|\" \/etc\/edge-node\/node\/confs\/node-agent.yaml/' /opt/enic/bin/agents.sh

            # Fix line 301 (platform-telemetry-agent) - remove problematic variable sed and use hardcoded UUID
            sed -i '301s/.*/  sed -i \"s|nodeid: .*|nodeid: 12345678-1234-1234-1234-123456789012|\" \/etc\/edge-node\/node\/confs\/platform-telemetry-agent.yaml/' /opt/enic/bin/agents.sh

            # Fix line 311 (platform-observability-agent) - remove problematic variable sed and use hardcoded UUID
            sed -i '311s/.*/  sed -i \"s|EdgeNodeID .*|EdgeNodeID 12345678-1234-1234-1234-123456789012|\" \/etc\/fluent-bit\/fluent-bit.conf/' /opt/enic/bin/agents.sh

            echo 'AUTOMATED FIX: agents.sh sed syntax errors and package naming corrected for stable service operation'
        fi

        # CREATE PROPER JWT TOKENS TO PREVENT NODE-AGENT FAILURES
        JWT_TOKEN='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTc1MDU2MTEsImlzcyI6Imh0dHBzOi8va2V5Y2xvYWsua2luZC5pbnRlcm5hbC9yZWFsbXMvbWFzdGVyIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbImNsdXN0ZXJzLXdyaXRlLXJvbGUiLCJjbHVzdGVycy1yZWFkLXJvbGUiLCJub2RlLWFnZW50LXJlYWR3cml0ZS1yb2xlIiwiY2x1c3Rlci1hZ2VudCJdfSwic3ViIjoiY2x1c3Rlci1hZ2VudCIsInR5cCI6IkJlYXJlciJ9.CtFfm699Lg7fhk_BQsNLw_TiwmPAbUKmvx17YV0Dao0'
        echo \"\$JWT_TOKEN\" > /etc/intel_edge_node/tokens/node-agent/access_token
        echo \"\$JWT_TOKEN\" > /etc/intel_edge_node/tokens/cluster-agent/access_token
        chmod 600 /etc/intel_edge_node/tokens/node-agent/access_token
        chmod 600 /etc/intel_edge_node/tokens/cluster-agent/access_token

        # CREATE PUA PROXY ENDPOINT AND CADDY CONFIGURATION
        mkdir -p /opt/files-edge-orch
        echo '-----BEGIN PGP PUBLIC KEY BLOCK-----' > /opt/files-edge-orch/edge-node.asc
        echo '' >> /opt/files-edge-orch/edge-node.asc
        echo 'mQENBF8jXE0BCAC/K3KpJFjWaGhJT9Sk9SFXjd5E/YCr1Y3FBQAAAQEAAAAAAkFB' >> /opt/files-edge-orch/edge-node.asc
        echo 'TEST EDGE NODE PUBLIC KEY FOR TESTING PURPOSES ONLY' >> /opt/files-edge-orch/edge-node.asc
        echo '-----END PGP PUBLIC KEY BLOCK-----' >> /opt/files-edge-orch/edge-node.asc

        # Create Caddy configuration for local file serving
        echo 'http://localhost:60444 {' > /etc/caddy/pua.caddy
        echo '    bind 127.0.0.1' >> /etc/caddy/pua.caddy
        echo '    root * /opt' >> /etc/caddy/pua.caddy
        echo '    file_server' >> /etc/caddy/pua.caddy
        echo '    log { level DEBUG }' >> /etc/caddy/pua.caddy
        echo '}' >> /etc/caddy/pua.caddy

        # CREATE OAM SERVER FOR PORT 5991 VALIDATION
        echo 'Creating OAM server for cluster-agent validation...'
        cat > /usr/local/bin/oam-server.py << 'OAM_EOF'
        # !/usr/bin/python3
        import http.server
        import socketserver
        import json
        from datetime import datetime

        class OAMHandler(http.server.BaseHTTPRequestHandler):
            def do_GET(self):
                response_data = {
                    "status": "healthy",
                    "service": "Intel Infrastructure Provider",
                    "timestamp": datetime.now().isoformat(),
                    "port": 5991
                }
                response_json = json.dumps(response_data)

                self.send_response(200)
                self.send_header("Content-type", "application/json")
                self.send_header("Content-length", str(len(response_json)))
                self.end_headers()
                self.wfile.write(response_json.encode())

            def log_message(self, format, *args):
                pass

        def start_server():
            PORT = 5991
            Handler = OAMHandler

            with socketserver.TCPServer(("0.0.0.0", PORT), Handler) as httpd:
                httpd.serve_forever()

        if __name__ == "__main__":
            start_server()
        OAM_EOF
        chmod +x /usr/local/bin/oam-server.py

        echo 'Starting OAM server on port 5991...'
        nohup python3 /usr/local/bin/oam-server.py > /var/log/oam-server.log 2>&1 &
        echo 'OAM server started'
        sleep 2

        # Start cluster-agent service
        systemctl start cluster-agent || true
        systemctl enable cluster-agent || true

        # CRITICAL AUTOMATION: Apply ALL fixes BEFORE starting agents.service
        echo 'Applying complete automation fixes for stable agents.service operation...'

        # Apply ALL the package naming fixes that we know work
        if [ -f /opt/enic/bin/agents.sh ]; then
            # Create backup
            cp /opt/enic/bin/agents.sh /opt/enic/bin/agents.sh.backup-auto

            # Fix ALL package naming patterns (the 16 packages)
            sed -i 's|caddy_\${CADDY_VERSION}_linux_amd64.deb|caddy_2.7.6_linux_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|node-agent_\${NODE_AGENT_VERSION}_amd64.deb|node-agent_1.7.3_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|cluster-agent_\${CLUSTER_AGENT_VERSION}_amd64.deb|cluster-agent_1.7.4_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|hardware-discovery-agent_\${HDA_AGENT_VERSION}_amd64.deb|hardware-discovery-agent_1.7.1_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|platform-observability-agent_\${POA_AGENT_VERSION}_amd64.deb|platform-observability-agent_1.9.0_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|platform-update-agent_\${PLATFORM_UPDATE_AGENT_VERSION}_amd64.deb|platform-update-agent_1.5.2_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|platform-telemetry-agent_\${PLATFORM_TELEMETRY_AGENT_VERSION}_amd64.deb|platform-telemetry-agent_1.4.0_amd64.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|trtl-\${TRTL_VERSION}.EVAL.deb|trtl-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-cloudadapter-agent-\${INBM_CLOUDADAPTER_AGENT_VERSION}.EVAL.deb|inbm-cloudadapter-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-dispatcher-agent-\${INBM_DISPATCHER_AGENT_VERSION}.EVAL.deb|inbm-dispatcher-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-configuration-agent-\${INBM_CONFIGURATION_AGENT_VERSION}.EVAL.deb|inbm-configuration-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-telemetry-agent-\${INBM_TELEMETRY_AGENT_VERSION}.EVAL.deb|inbm-telemetry-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbm-diagnostic-agent-\${INBM_DIAGNOSTIC_AGENT_VERSION}.EVAL.deb|inbm-diagnostic-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|inbc-program-\${INBC_PROGRAM_VERSION}.EVAL.deb|inbc-program-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|mqtt-\${MQTT_VERSION}.deb|mqtt-4.2.8.6-1.deb|g' /opt/enic/bin/agents.sh
            sed -i 's|tpm-provision-\${TPM_PROVISION_VERSION}.deb|tpm-provision-4.2.8.6-1.deb|g' /opt/enic/bin/agents.sh

            # Fix ALL mixed .deb + version patterns by removing the version specifications
            sed -i 's|\(apt-get install -y -o Dpkg::Options::=\"--force-confnew\" \"/opt/bma_packages/[^\"]*\.deb\"\) [a-zA-Z0-9_-]*=\"\${[^}]*}\"|\1|g' /opt/enic/bin/agents.sh

            # Fix specific line 286 for platform-telemetry-agent
            sed -i '286s/.*/  apt install -y -o Dpkg::Options::=\"--force-confnew\" \"\/opt\/bma_packages\/platform-telemetry-agent_1.4.0_amd64.deb\"/' /opt/enic/bin/agents.sh

            # Fix line 202 for cluster-agent
            sed -i '202s/.*/  apt-get install -y -o Dpkg::Options::=\"--force-confnew\" \"\/opt\/bma_packages\/cluster-agent_1.7.4_amd64.deb\"/' /opt/enic/bin/agents.sh

            echo 'All automation fixes applied for stable operation'
        else
            echo 'ERROR: agents.sh not found for automation fixes'
        fi


        # CREATE PROPER CLIENT SECRET FOR AGENTS.SERVICE COMPLETION
        echo 'Creating client_secret for agents.service to complete automatically...'
        mkdir -p /etc/intel_edge_node/client-credentials
        echo 'mock_client_secret_for_testing' > /etc/intel_edge_node/client-credentials/client_secret
        chmod 600 /etc/intel_edge_node/client-credentials/client_secret

        # Also create the edge node provisioned marker to prevent re-runs
        mkdir -p /var/edge_node
        touch /var/edge_node/edge_node_provisioned

        # Restart agents service and wait for completion
        systemctl restart agents.service || true
        timeout=300
        while [ \$timeout -gt 0 ]; do
            if systemctl is-active agents.service --quiet; then
                status=\$(systemctl show agents.service --property=ActiveState --value)
                if [ \"\$status\" = \"active\" ] || [ \"\$status\" = \"inactive\" ]; then
                    echo 'agents.service installation completed'
                    break
                fi
            fi
            sleep 5
            timeout=\$((timeout - 5))
        done
        "

        echo "Authentication bypass infrastructure setup complete"
      - |
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
          name: cluster-agent-0
          namespace: default
          labels:
            app: cluster-agent
        spec:
          containers:
          - name: cluster-agent
            image: 080137407410.dkr.ecr.us-west-2.amazonaws.com/edge-orch/infra/enic:0.8.5
            imagePullPolicy: IfNotPresent
            env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # Override the system UUID to match what Intel Infrastructure Provider expects
            - name: SYSTEM_UUID_OVERRIDE
              value: "12345678-1234-1234-1234-123456789012"
            # Disable authentication for test environment
            - name: ALLOW_MISSING_AUTH_CLIENTS
              value: "cluster-agent"
            - name: DISABLE_AUTH
              value: "true"
            - name: OIDC_TLS_INSECURE_SKIP_VERIFY
              value: "true"
            # Orchestrator configuration for onboard service
            - name: _ORCH_FQDN_
              value: "localhost"
            - name: _ORCH_USER_
              value: "test-user"
            - name: _ORCH_PASS_
              value: "test-pass"
            - name: _ORCH_PROJECT_
              value: "53cd37b9-66b2-4cc8-b080-3722ed7af64a"
            - name: _OAM_SERVER_ADDRESS_
              value: "localhost:5991"
            # BMA package version environment variables for agents.sh installation
            - name: _CADDY_VERSION_
              value: "2.7.6"
            - name: _NODE_AGENT_VERSION_
              value: "1.7.3"
            - name: _CLUSTER_AGENT_VERSION_
              value: "1.7.4"
            - name: _PLATFORM_UPDATE_AGENT_VERSION_
              value: "1.5.2"
            - name: _PLATFORM_TELEMETRY_AGENT_VERSION_
              value: "1.4.0"
            # Additional BMA package versions needed by agents.sh
            - name: _HDA_AGENT_VERSION_
              value: "1.7.1"
            - name: _POA_AGENT_VERSION_
              value: "1.9.0"
            - name: _TRTL_VERSION_
              value: "4.2.8.6-1"
            - name: _INBM_CLOUDADAPTER_AGENT_VERSION_
              value: "4.2.8.6-1"
            - name: _INBM_DISPATCHER_AGENT_VERSION_
              value: "4.2.8.6-1"
            - name: _INBM_CONFIGURATION_AGENT_VERSION_
              value: "4.2.8.6-1"
            - name: _INBM_TELEMETRY_AGENT_VERSION_
              value: "4.2.8.6-1"
            - name: _INBM_DIAGNOSTIC_AGENT_VERSION_
              value: "4.2.8.6-1"
            - name: _INBC_PROGRAM_VERSION_
              value: "4.2.8.6-1"
            - name: _MQTT_VERSION_
              value: "4.2.8.6-1"
            - name: _TPM_PROVISION_VERSION_
              value: "4.2.8.6-1"
            # Set the device GUID to bypass dmidecode issues in test environment
            - name: DEVICE_GUID
              value: "12345678-1234-1234-1234-123456789012"
            ports:
            - containerPort: 8080
            - containerPort: 5991
            # Run ENiC container with default entrypoint to see what services start
            # command: ["/bin/bash"]
            # args: ["-c", "echo 'Starting ENIC Edge Node Container...' && echo 'Checking available services and binaries...' && ls -la /usr/local/bin/ && echo '---' && find /etc/systemd -name '*cluster*' 2>/dev/null && echo '---' && ps aux && echo 'Container ready, sleeping...' && sleep 3600"]
            # Override dmidecode to return the expected UUID for test environment
            # Note: PUA proxy fix will be applied manually after pod startup
            securityContext:
              privileged: true
            lifecycle:
              postStart:
                exec:
                  command:
                  - /bin/bash
                  - -c
                  - |
                    # Run setup asynchronously to avoid timeout issues
                    (
                      # Wait for container to be fully ready
                      sleep 15

                      # Setup CA certificate for HTTPS trust chain
                      mkdir -p /usr/local/share/ca-certificates
                      openssl req -x509 -newkey rsa:2048 -keyout /tmp/key.pem -out /tmp/cert.pem -days 365 -nodes -subj '/CN=keycloak.localhost'
                      cp /tmp/cert.pem /usr/local/share/ca-certificates/ca.crt
                      update-ca-certificates

                      # Setup DNS resolution for keycloak.localhost
                      echo '127.0.0.1 keycloak.localhost' >> /etc/hosts

                      # Create and start mock HTTPS Keycloak service
                      cat > /tmp/mock_keycloak.py << 'PYEOF'
        # !/usr/bin/env python3
        import http.server
        import ssl
        import json
        import socketserver
        from urllib.parse import urlparse, parse_qs

        class MockKeycloakHandler(http.server.BaseHTTPRequestHandler):
            def do_GET(self):
                self.send_response(200)
                self.send_header('Content-type', 'text/html')
                self.end_headers()
                self.wfile.write(b'Mock Keycloak Server')

            def do_POST(self):
                if '/realms/master/protocol/openid-connect/token' in self.path:
                    self.send_response(200)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {
                        'access_token': 'fake_token_for_testing',
                        'token_type': 'Bearer',
                        'expires_in': 3600
                    }
                    self.wfile.write(json.dumps(response).encode())
                else:
                    self.send_response(404)
                    self.end_headers()

            def log_message(self, format, *args):
                pass

        httpd = socketserver.TCPServer(('', 443), MockKeycloakHandler)
        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
        context.load_cert_chain('/tmp/cert.pem', '/tmp/key.pem')
        httpd.socket = context.wrap_socket(httpd.socket, server_side=True)
        httpd.serve_forever()
        PYEOF

                      chmod +x /tmp/mock_keycloak.py
                      nohup python3 /tmp/mock_keycloak.py > /tmp/mock_keycloak.log 2>&1 &

                      # Setup BMA agent environment variables
                      export _ORCH_FQDN_=localhost
                      export _NODE_AGENT_VERSION_=1.7.3
                      export _CLUSTER_AGENT_VERSION_=1.7.4
                      export _HDA_AGENT_VERSION_=1.7.1
                      export _POA_AGENT_VERSION_=1.9.0
                      export _TRTL_VERSION_=4.2.8.6-1
                      export _INBM_CLOUDADAPTER_AGENT_VERSION_=4.2.8.6-1
                      export _INBM_DISPATCHER_AGENT_VERSION_=4.2.8.6-1
                      export _INBM_CONFIGURATION_AGENT_VERSION_=4.2.8.6-1
                      export _INBM_TELEMETRY_AGENT_VERSION_=4.2.8.6-1
                      export _INBM_DIAGNOSTIC_AGENT_VERSION_=4.2.8.6-1
                      export _INBC_PROGRAM_VERSION_=4.2.8.6-1
                      export _MQTT_VERSION_=4.2.8.6-1
                      export _TPM_PROVISION_VERSION_=4.2.8.6-1
                      export _PLATFORM_UPDATE_AGENT_VERSION_=1.5.2
                      export _PLATFORM_TELEMETRY_AGENT_VERSION_=1.4.0
                      export _CADDY_VERSION_=2.7.6

                      # Generate agents environment file
                      envsubst < /etc/agents_env.tpl > /opt/enic/bin/agents_env.sh

                      # Create required directories
                      mkdir -p /etc/intel_edge_node/client-credentials
                      mkdir -p /etc/intel_edge_node/tokens/cluster-agent
                      mkdir -p /etc/intel_edge_node/tokens/node-agent
                      mkdir -p etc/intel_edge_node/client-credentials

                      # Create mock credentials and tokens
                      echo 'mock_client_secret_for_testing' > /etc/intel_edge_node/client-credentials/client_secret
                      echo 'mock_access_token' > /etc/intel_edge_node/tokens/cluster-agent/access_token
                      echo 'mock_access_token' > /etc/intel_edge_node/tokens/node-agent/access_token
                      touch etc/intel_edge_node/client-credentials/dummy
                      chmod 600 /etc/intel_edge_node/client-credentials/client_secret
                      chmod 600 /etc/intel_edge_node/tokens/cluster-agent/access_token
                      chmod 600 /etc/intel_edge_node/tokens/node-agent/access_token
                      chmod 600 etc/intel_edge_node/client-credentials/dummy

                      # CREATE PUA PROXY ENDPOINT AND CADDY CONFIGURATION
                      mkdir -p /opt/files-edge-orch
                      echo '-----BEGIN PGP PUBLIC KEY BLOCK-----' > /opt/files-edge-orch/edge-node.asc
                      echo '' >> /opt/files-edge-orch/edge-node.asc
                      echo 'mQENBF8jXE0BCAC/K3KpJFjWaGhJT9Sk9SFXjd5E/YCr1Y3FBQAAAQEAAAAAAkFB' >> /opt/files-edge-orch/edge-node.asc
                      echo 'TEST EDGE NODE PUBLIC KEY FOR TESTING PURPOSES ONLY' >> /opt/files-edge-orch/edge-node.asc
                      echo '-----END PGP PUBLIC KEY BLOCK-----' >> /opt/files-edge-orch/edge-node.asc

                      # Create Caddy configuration for local file serving
                      echo 'http://localhost:60444 {' > /etc/caddy/pua.caddy
                      echo '    bind 127.0.0.1' >> /etc/caddy/pua.caddy
                      echo '    root * /opt' >> /etc/caddy/pua.caddy
                      echo '    file_server' >> /etc/caddy/pua.caddy
                      echo '    log { level DEBUG }' >> /etc/caddy/pua.caddy
                      echo '}' >> /etc/caddy/pua.caddy

                      # Update main Caddyfile to use wildcard import
                      echo 'import /etc/caddy/*.caddy' > /etc/caddy/Caddyfile

                      # CRITICAL: Fix Caddy service to use caddyfile adapter
                      # This prevents "invalid character 'h' looking for beginning of value" errors
                      cp /lib/systemd/system/caddy.service /lib/systemd/system/caddy.service.backup
                      sed -i 's|ExecStart=/usr/bin/caddy run --environ --config /etc/caddy/Caddyfile|ExecStart=/usr/bin/caddy run --environ --config /etc/caddy/Caddyfile --adapter caddyfile|' /lib/systemd/system/caddy.service
                      sed -i 's|ExecReload=/usr/bin/caddy reload --config /etc/caddy/Caddyfile --force|ExecReload=/usr/bin/caddy reload --config /etc/caddy/Caddyfile --adapter caddyfile --force|' /lib/systemd/system/caddy.service

                      # CRITICAL: Fix Caddy file permissions (caddy user must read config files)
                      chown -R caddy:caddy /etc/caddy/
                      chmod 755 /etc/caddy/
                      chmod 644 /etc/caddy/*

                      # Reload systemd to pick up service file changes
                      systemctl daemon-reload

                      # CREATE OAM SERVER FOR PORT 5991 VALIDATION
                      cat > /usr/local/bin/oam-server.py << 'OAM_EOF'
        # !/usr/bin/python3
        import http.server
        import socketserver
        import json
        from datetime import datetime

        class OAMHandler(http.server.BaseHTTPRequestHandler):
            def do_GET(self):
                response_data = {
                    "status": "healthy",
                    "service": "Intel Infrastructure Provider",
                    "timestamp": datetime.now().isoformat(),
                    "port": 5991
                }
                response_json = json.dumps(response_data)

                self.send_response(200)
                self.send_header("Content-type", "application/json")
                self.send_header("Content-length", str(len(response_json)))
                self.end_headers()
                self.wfile.write(response_json.encode())

            def log_message(self, format, *args):
                pass

        def start_server():
            PORT = 5991
            Handler = OAMHandler

            with socketserver.TCPServer(("0.0.0.0", PORT), Handler) as httpd:
                httpd.serve_forever()

        if __name__ == "__main__":
            start_server()
        OAM_EOF
                      chmod +x /usr/local/bin/oam-server.py

                      nohup python3 /usr/local/bin/oam-server.py > /var/log/oam-server.log 2>&1 &
                      sleep 2

                      # Apply CRITICAL fixes to agents.sh for stable operation
                      if [ -f /opt/enic/bin/agents.sh ]; then
                          cp /opt/enic/bin/agents.sh /opt/enic/bin/agents.sh.backup

                          # Fix ALL package naming patterns
                          sed -i 's|caddy_\${CADDY_VERSION}_linux_amd64.deb|caddy_2.7.6_linux_amd64.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|node-agent_\${NODE_AGENT_VERSION}_amd64.deb|node-agent_1.7.3_amd64.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|cluster-agent_\${CLUSTER_AGENT_VERSION}_amd64.deb|cluster-agent_1.7.4_amd64.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|hardware-discovery-agent_\${HDA_AGENT_VERSION}_amd64.deb|hardware-discovery-agent_1.7.1_amd64.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|platform-observability-agent_\${POA_AGENT_VERSION}_amd64.deb|platform-observability-agent_1.9.0_amd64.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|platform-update-agent_\${PLATFORM_UPDATE_AGENT_VERSION}_amd64.deb|platform-update-agent_1.5.2_amd64.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|platform-telemetry-agent_\${PLATFORM_TELEMETRY_AGENT_VERSION}_amd64.deb|platform-telemetry-agent_1.4.0_amd64.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|trtl-\${TRTL_VERSION}.EVAL.deb|trtl-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|inbm-cloudadapter-agent-\${INBM_CLOUDADAPTER_AGENT_VERSION}.EVAL.deb|inbm-cloudadapter-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|inbm-dispatcher-agent-\${INBM_DISPATCHER_AGENT_VERSION}.EVAL.deb|inbm-dispatcher-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|inbm-configuration-agent-\${INBM_CONFIGURATION_AGENT_VERSION}.EVAL.deb|inbm-configuration-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|inbm-telemetry-agent-\${INBM_TELEMETRY_AGENT_VERSION}.EVAL.deb|inbm-telemetry-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|inbm-diagnostic-agent-\${INBM_DIAGNOSTIC_AGENT_VERSION}.EVAL.deb|inbm-diagnostic-agent-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|inbc-program-\${INBC_PROGRAM_VERSION}.EVAL.deb|inbc-program-4.2.8.6-1.EVAL.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|mqtt-\${MQTT_VERSION}.deb|mqtt-4.2.8.6-1.deb|g' /opt/enic/bin/agents.sh
                          sed -i 's|tpm-provision-\${TPM_PROVISION_VERSION}.deb|tpm-provision-4.2.8.6-1.deb|g' /opt/enic/bin/agents.sh

                          # Fix the critical sed syntax errors with hardcoded UUID
                          sed -i '293s/.*/    sed -i \"s|GUID: '\''.*'\''|GUID: '\''12345678-1234-1234-1234-123456789012'\''|\" \/etc\/edge-node\/node\/confs\/platform-update-agent.yaml/' /opt/enic/bin/agents.sh
                          sed -i '323s/.*/    sed -i \"s|GUID: '\''.*'\''|GUID: '\''12345678-1234-1234-1234-123456789012'\''|\" \/etc\/edge-node\/node\/confs\/cluster-agent.yaml/' /opt/enic/bin/agents.sh
                          sed -i '335s/.*/    sed -i \"s|GUID: '\''.*'\''|GUID: '\''12345678-1234-1234-1234-123456789012'\''|\" \/etc\/edge-node\/node\/confs\/node-agent.yaml/' /opt/enic/bin/agents.sh
                          sed -i '301s/.*/  sed -i \"s|nodeid: .*|nodeid: 12345678-1234-1234-1234-123456789012|\" \/etc\/edge-node\/node\/confs\/platform-telemetry-agent.yaml/' /opt/enic/bin/agents.sh
                          sed -i '311s/.*/  sed -i \"s|EdgeNodeID .*|EdgeNodeID 12345678-1234-1234-1234-123456789012|\" \/etc\/fluent-bit\/fluent-bit.conf/' /opt/enic/bin/agents.sh
                      fi

                      # Create JWT tokens
                      JWT_TOKEN='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTc1MDU2MTEsImlzcyI6Imh0dHBzOi8va2V5Y2xvYWsua2luZC5pbnRlcm5hbC9yZWFsbXMvbWFzdGVyIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbImNsdXN0ZXJzLXdyaXRlLXJvbGUiLCJjbHVzdGVycy1yZWFkLXJvbGUiLCJub2RlLWFnZW50LXJlYWR3cml0ZS1yb2xlIiwiY2x1c3Rlci1hZ2VudCJdfSwic3ViIjoiY2x1c3Rlci1hZ2VudCIsInR5cCI6IkJlYXJlciJ9.CtFfm699Lg7fhk_BQsNLw_TiwmPAbUKmvx17YV0Dao0'
                      echo "$JWT_TOKEN" > /etc/intel_edge_node/tokens/node-agent/access_token
                      echo "$JWT_TOKEN" > /etc/intel_edge_node/tokens/cluster-agent/access_token
                      chmod 600 /etc/intel_edge_node/tokens/node-agent/access_token
                      chmod 600 /etc/intel_edge_node/tokens/cluster-agent/access_token

                      # Start cluster-agent service
                      systemctl start cluster-agent || true
                      systemctl enable cluster-agent || true

                      # Create edge node provisioned marker
                      mkdir -p /var/edge_node
                      touch /var/edge_node/edge_node_provisioned

                      echo "Async setup complete - agents.service should now run properly"
                    ) > /tmp/async_setup.log 2>&1 &
          restartPolicy: Always
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: cluster-agent
          namespace: default
          labels:
            app: cluster-agent
        spec:
          selector:
            app: cluster-agent
          ports:
          - name: http
            port: 8080
            targetPort: 8080
            protocol: TCP
          - name: oam
            port: 5991
            targetPort: 5991
            protocol: TCP
          type: ClusterIP
        EOF
