# SPDX-FileCopyrightText: (C) 2026 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
---
name: Virtual Integration (VEN)

on:
  workflow_dispatch:
    inputs:
      deploy-orchestrator:
        description: "Deploy EMF orchestrator onto Kind"
        required: true
        type: boolean
        default: false
      orchestrator-deployment-type:
        description: "EMF deploy_kind deployment type"
        required: true
        type: choice
        default: minimal
        options:
          - minimal
          - all
      use-ven:
        description: "Run VEN prerequisites (dnsmasq + libvirt) before tests"
        required: true
        type: boolean
        default: true
      emf-branch:
        description: "Branch/tag/SHA to checkout edge-manageability-framework for ci/ven scripts"
        required: true
        type: string
        default: main

permissions:
  contents: read

jobs:
  virtual-integration:
    name: Run Golden Suite VEN tests (optional EMF deploy)
    runs-on: ubuntu-24.04-16core-64GB
    timeout-minutes: 180

    env:
      # Ensure any libvirt tooling uses the system URI.
      LIBVIRT_DEFAULT_URI: qemu:///system

    steps:
      - name: Checkout cluster-tests
        uses: actions/checkout@v6
        with:
          fetch-depth: 1
          persist-credentials: false

      - name: Mask secrets
        if: ${{ inputs.use-ven || inputs.deploy-orchestrator }}
        run: |
          echo "::add-mask::${{ secrets.SYS_EMF_GH_TOKEN }}"
          echo "::add-mask::${{ secrets.ORCH_DEFAULT_PASSWORD }}"
          echo "::add-mask::${{ secrets.SYS_DOCKERHUB_RO }}"
          echo "::add-mask::${{ secrets.SYS_DOCKERHUB_USERNAME }}"

      - name: Install system prerequisites (toolchain)
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            build-essential \
            ca-certificates \
            curl \
            git \
            libbz2-dev \
            libffi-dev \
            liblzma-dev \
            libncursesw5-dev \
            libreadline-dev \
            libsqlite3-dev \
            libssl-dev \
            tk-dev \
            xz-utils \
            zlib1g-dev

      - name: Cache asdf installs
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57  # v4.2.0
        with:
          path: |
            ~/.asdf/plugins
            ~/.asdf/installs
            ~/.asdf/downloads
            ~/.asdf/shims
          key: asdf-${{ runner.os }}-${{ hashFiles('.tool-versions') }}
          restore-keys: |
            asdf-${{ runner.os }}-

      - name: Cache Go modules
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57  # v4.2.0
        with:
          path: |
            ~/go/pkg/mod
            ~/.cache/go-build
          key: go-${{ runner.os }}-${{ hashFiles('go.sum') }}
          restore-keys: |
            go-${{ runner.os }}-

      - name: Install and activate asdf (ensure shims first)
        if: ${{ inputs.deploy-orchestrator }}
        run: |
          set -euo pipefail

          # EMF's mage targets install and expect tools (including kubectl) via asdf.
          # On GitHub-hosted runners, asdf may not be installed or activated for
          # non-interactive shells, causing mage to pick up the runner kubectl.
          # Ensure asdf is installed and ~/.asdf/shims is ahead of /usr/bin.

          ASDF_DIR="$HOME/.asdf"
          if [ ! -d "$ASDF_DIR" ]; then
            git clone https://github.com/asdf-vm/asdf.git "$ASDF_DIR" --branch v0.15.0
          fi

          # Persist for subsequent steps (including composite actions).
          echo "ASDF_DIR=$ASDF_DIR" >> "$GITHUB_ENV"
          echo "ASDF_DATA_DIR=$ASDF_DIR" >> "$GITHUB_ENV"

          # Prepend shims and bin to PATH for subsequent steps.
          echo "$ASDF_DIR/shims" >> "$GITHUB_PATH"
          echo "$ASDF_DIR/bin" >> "$GITHUB_PATH"

          # Verify asdf works in the current step.
          . "$ASDF_DIR/asdf.sh"
          asdf --version

      - name: Install toolchain deps
        run: |
          # `make deps` uses `go install` for tools like `mage`. Ensure the default
          # Go install location is on PATH during this step and subsequent steps.
          echo "$HOME/go/bin" >> $GITHUB_PATH
          export PATH="$PATH:$HOME/go/bin"
          make deps

      - name: Run lint
        run: |
          make lint

      - name: Checkout EMF (ci/ven scripts)
        if: ${{ inputs.use-ven }}
        uses: actions/checkout@v6
        with:
          repository: open-edge-platform/edge-manageability-framework
          ref: ${{ inputs.emf-branch }}
          path: emf
          sparse-checkout: ci/ven
          token: ${{ secrets.SYS_EMF_GH_TOKEN }}
          fetch-depth: 1
          persist-credentials: false

      - name: Verify EMF VEN scripts exist
        if: ${{ inputs.use-ven }}
        run: |
          if [ ! -d emf/ci/ven ]; then
            echo "::error::EMF ref '${{ inputs.emf-branch }}' does not contain ci/ven. Set emf-branch to a ref that includes ci/ven (e.g. main)."
            echo "Workspace EMF contents:" 
            ls -la emf || true
            ls -la emf/ci || true
            exit 1
          fi

      - name: Preserve VEN scripts
        if: ${{ inputs.use-ven }}
        run: |
          cp -r emf/ci/ven /tmp/ven-scripts
          ls -la /tmp/ven-scripts/

      - name: Install DNSmasq (VEN)
        if: ${{ inputs.use-ven }}
        run: |
          cd /tmp/ven-scripts
          ./dnsmasq-setup.sh "kind.internal" setup

      - name: Install Libvirt (VEN)
        if: ${{ inputs.use-ven }}
        run: |
          cd /tmp/ven-scripts
          ./libvirt-setup.sh

      - name: Install Argo CD CLI (required by EMF deploy_kind)
        if: ${{ inputs.deploy-orchestrator }}
        run: |
          set -euo pipefail
          # Install the latest stable Argo CD CLI.
          ARGOCD_VERSION="$(curl -fsSL https://raw.githubusercontent.com/argoproj/argo-cd/stable/VERSION)"
          echo "Resolved Argo CD CLI version: v${ARGOCD_VERSION}"
          echo "::notice::Resolved Argo CD CLI version: v${ARGOCD_VERSION}"
          curl -fsSL -o /tmp/argocd "https://github.com/argoproj/argo-cd/releases/download/v${ARGOCD_VERSION}/argocd-linux-amd64"
          sudo install -m 0755 /tmp/argocd /usr/local/bin/argocd
          argocd version --client

      - name: Install kubectl stdout sanitizer (workaround for EMF mage JSON parsing)
        if: ${{ inputs.deploy-orchestrator }}
        run: |
          set -euo pipefail

          WRAP_DIR="${RUNNER_TEMP}/kubectl-wrap/bin"
          mkdir -p "$WRAP_DIR"

          cat >"$WRAP_DIR/kubectl" <<'EOF'
          #!/usr/bin/env bash
          set -euo pipefail

          # Resolve the *next* kubectl in PATH (avoids recursion and avoids pinning
          # to the runner kubectl when asdf installs a pinned kubectl later).
          self_dir="$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" && pwd -P)"
          filtered_path="$(printf '%s' "${PATH}" | tr ':' '\n' | awk -v s="$self_dir" '$0 != s' | paste -sd: -)"
          REAL_KUBECTL="$(PATH="$filtered_path" command -v kubectl || true)"
          if [ -z "${REAL_KUBECTL}" ]; then
            echo "kubectl not found in PATH (after filtering wrapper dir)" >&2
            exit 127
          fi

          # If kubectl is being used interactively (e.g. exec -it / attach), do not
          # capture output or sanitize anything. This preserves TTY behavior.
          for a in "$@"; do
            case "$a" in
              exec|attach)
                interactive_cmd=1
                ;;
              -t|--tty|-i|--stdin|-it|-ti)
                interactive_flag=1
                ;;
            esac
          done
          if [ "${interactive_cmd:-0}" = "1" ] && [ "${interactive_flag:-0}" = "1" ]; then
            exec "${REAL_KUBECTL}" "$@"
          fi

          stdout_file="$(mktemp)"
          stderr_file="$(mktemp)"
          trap 'rm -f "$stdout_file" "$stderr_file"' EXIT

          "${REAL_KUBECTL}" "$@" >"$stdout_file" 2>"$stderr_file" || rc=$?
          rc="${rc:-0}"

          # EMF mage targets parse kubectl output as strict JSON via gojq. On some
          # runners, kubectl can emit klog-style info lines (starting with 'I....')
          # to STDOUT when verbosity is enabled, corrupting the JSON stream.
          if grep -qE '^[IWEF][0-9]{4}' "$stdout_file"; then
            # If this looks like structured output, drop everything before the first
            # JSON object/array or YAML document start.
            if grep -qE '^[[:space:]]*[{[]' "$stdout_file"; then
              awk 'BEGIN{p=0} /^[[:space:]]*[{[]/{p=1} {if(p)print}' "$stdout_file" >"${stdout_file}.clean" || true
              mv "${stdout_file}.clean" "$stdout_file"
            elif grep -qE '^[[:space:]]*apiVersion:' "$stdout_file"; then
              awk 'BEGIN{p=0} /^[[:space:]]*apiVersion:/{p=1} {if(p)print}' "$stdout_file" >"${stdout_file}.clean" || true
              mv "${stdout_file}.clean" "$stdout_file"
            fi

            # As a last pass, remove any remaining klog-style lines.
            grep -vE '^[IWEF][0-9]{4}' "$stdout_file" >"${stdout_file}.clean" || true
            mv "${stdout_file}.clean" "$stdout_file"
          fi

          cat "$stdout_file"
          cat "$stderr_file" >&2
          exit "$rc"
          EOF

          chmod +x "$WRAP_DIR/kubectl"
          echo "$WRAP_DIR" >> "$GITHUB_PATH"

          echo "kubectl wrapper installed at: $WRAP_DIR/kubectl"

#      - name: Verify Docker Hub credentials (required for EMF deploy_kind)
#        if: ${{ inputs.deploy-orchestrator }}
#        run: |
#          set -euo pipefail
#          if [ -z "${{ secrets.SYS_DOCKERHUB_USERNAME }}" ] || [ -z "${{ secrets.SYS_DOCKERHUB_RO }}" ]; then
#            echo "::error::Missing SYS_DOCKERHUB_USERNAME and/or SYS_DOCKERHUB_RO secrets."
#            echo "EMF deploy_kind pulls multiple images from docker.io and may hit rate limits without auth."
#            exit 1
#          fi

      - name: Deploy Kind Orchestrator (EMF baseline)
        if: ${{ inputs.deploy-orchestrator }}
        id: deploy_emf
        continue-on-error: true
        uses: open-edge-platform/edge-manageability-framework/.github/actions/deploy_kind@3a2d9869daa26e1925daf159dabb7307fc24e3ed
        env:
          # EMF's kind deployment flow can optionally run a local docker-compose based
          # router (Traefik) to expose services via hostnames. GitHub-hosted runners
          # don't reliably provide a `docker-compose` binary, and the router isn't
          # required for cluster-tests (we configure DNSmasq later).
          SKIP_ROUTER: "1"
          # Ensure EMF's mage targets don't enable kubectl debug output that can
          # pollute JSON parsing in post-deploy generators.
          MAGEFILE_DEBUG: "0"
#          DOCKERHUB_USERNAME: ${{ secrets.SYS_DOCKERHUB_USERNAME }}
#          DOCKERHUB_TOKEN: ${{ secrets.SYS_DOCKERHUB_RO }}
        with:
          orch_version: ${{ inputs.emf-branch }}
          orch_password: ${{ secrets.ORCH_DEFAULT_PASSWORD }}
          token: ${{ secrets.SYS_EMF_GH_TOKEN }}
          deployment_type: ${{ inputs.orchestrator-deployment-type }}

      - name: Validate Orchestrator endpoints (post EMF deploy_kind)
        if: ${{ inputs.deploy-orchestrator }}
        run: |
          set -euo pipefail

          if [ "${{ steps.deploy_emf.outcome }}" != "success" ]; then
            echo "::warning::EMF deploy_kind step ended with outcome '${{ steps.deploy_emf.outcome }}' (continuing). Verifying cluster health manually..."
          fi

          kind export kubeconfig --name kind

          # Wait for API connectivity.
          for i in {1..60}; do
            if kubectl get nodes >/dev/null 2>&1; then
              break
            fi
            sleep 2
          done
          kubectl get nodes -o wide

          # Wait for required service IPs to be assigned.
          await_lb_ip() {
            local ns="$1"
            local svc="$2"
            local timeout_secs="$3"
            local start
            start="$(date +%s)"
            while true; do
              ip="$(kubectl -n "$ns" get svc "$svc" -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)"
              if [ -n "$ip" ]; then
                echo "${ns}/${svc} external IP: ${ip}"
                return 0
              fi
              now="$(date +%s)"
              if [ $((now - start)) -ge "$timeout_secs" ]; then
                echo "::error::Timed out waiting for LoadBalancer IP on ${ns}/${svc}"
                kubectl -n "$ns" get svc "$svc" -o wide || true
                kubectl -n "$ns" describe svc "$svc" || true
                return 1
              fi
              sleep 10
            done
          }

          await_lb_ip orch-gateway traefik 900
          await_lb_ip orch-boots ingress-haproxy-kubernetes-ingress 900
          await_lb_ip argocd argocd-server 900
          await_lb_ip gitea gitea-http 900

          # Sanity-check the exact kubectl JSON outputs that EMF's mage generators parse.
          # If these aren't strict JSON, mage will fail with: invalid character 'I'...
          echo "Validating kubectl JSON for EMF mage generators..."
          kubectl --v=1 -n orch-gateway get configmap kubernetes-docker-internal -o json | jq -e . >/dev/null

          # Some resources are optional / deployment-type dependent. Only validate
          # JSON output if the object exists.
          if kubectl --v=1 -n orch-boots get cert tls-boots -o json >/dev/null 2>&1; then
            kubectl --v=1 -n orch-boots get cert tls-boots -o json | jq -e . >/dev/null
          else
            echo "::warning::Skipping JSON check: orch-boots/cert tls-boots not found"
          fi

          if kubectl -v 1 -n orch-sre get cert kubernetes-docker-internal -o json >/dev/null 2>&1; then
            kubectl -v 1 -n orch-sre get cert kubernetes-docker-internal -o json | jq -e . >/dev/null
          else
            echo "::warning::Skipping JSON check: orch-sre/cert kubernetes-docker-internal not found"
          fi

      - name: Configure DNSmasq (VEN)
        if: ${{ inputs.use-ven && inputs.deploy-orchestrator }}
        run: |
          # dnsmasq-setup.sh "config" needs kubectl access and expects the orchestrator
          # LoadBalancer services (argocd/orch-boots/orch-gateway) to exist.
          kind export kubeconfig --name kind

          # Wait for kube-apiserver connectivity (avoids kubectl falling back to localhost:8080).
          for i in {1..60}; do
            if kubectl get nodes >/dev/null 2>&1; then
              break
            fi
            sleep 2
          done
          kubectl get nodes -o wide

          # Wait for expected namespaces/services to exist and obtain external IPs.
          for i in {1..120}; do
            if kubectl get svc -n orch-boots >/dev/null 2>&1 && kubectl get svc -n orch-gateway >/dev/null 2>&1; then
              if kubectl get svc -n orch-boots | grep -q LoadBalancer && kubectl get svc -n orch-gateway | grep -q LoadBalancer; then
                break
              fi
            fi
            sleep 5
          done

          kubectl get svc -n orch-boots -o wide
          kubectl get svc -n orch-gateway -o wide
          cd /tmp/ven-scripts
          ./dnsmasq-setup.sh "kind.internal" config

      - name: Setup Orchestrator CA (post EMF deploy_kind)
        if: ${{ inputs.deploy-orchestrator }}
        run: |
          set -euo pipefail
          # Generate orch-ca.crt from the live cluster and install it into the
          # system trust store. Required so Robot Framework (and curl) trust the
          # orchestrator TLS certificate when hitting HTTPS endpoints.
          mage gen:orchCa deploy:orchCa

      - name: Create default multi-tenancy setup (post EMF deploy_kind)
        if: ${{ inputs.deploy-orchestrator }}
        id: default-mt-setup
        run: |
          set -euo pipefail
          # Seeds Keycloak with the default org, project, and users that
          # Robot Framework needs to authenticate against the orchestrator.
          mage tenantUtils:createDefaultMtSetup
          echo "Orch org/project/users created!"

      - name: Checkout edge-manage-test-automation
        if: ${{ inputs.use-ven && inputs.deploy-orchestrator }}
        uses: actions/checkout@v6
        with:
          repository: open-edge-platform/edge-manage-test-automation
          ref: '0.3.3'
          path: edge-manage-test-automation
          submodules: recursive
          token: ${{ secrets.SYS_EMF_GH_TOKEN }}
          persist-credentials: false

      - name: Setup edge-manage-test-automation
        if: ${{ inputs.use-ven && inputs.deploy-orchestrator }}
        working-directory: edge-manage-test-automation
        run: |
          set -euo pipefail
          git submodule update --init --recursive
          make asdf-install
          make venv_edge-manage-test-automation
          pushd repos/ven/pico
          asdf install
          popd
          sudo apt-get install -y --no-install-recommends xsltproc

      - name: Deploy VEN and Edge Cluster via Golden Suite
        if: ${{ inputs.use-ven && inputs.deploy-orchestrator && steps.default-mt-setup.conclusion == 'success' }}
        id: deploy-ven
        working-directory: edge-manage-test-automation
        timeout-minutes: 60
        env:
          REQUESTS_CA_BUNDLE: /usr/local/share/ca-certificates/orch-ca.crt
          LIBVIRT_DEFAULT_URI: qemu:///system
          ORCH_DEFAULT_PASSWORD: ${{ secrets.ORCH_DEFAULT_PASSWORD }}
        run: |
          set -euo pipefail
          KC_ADMIN_PWD=$(kubectl -n orch-platform get secrets platform-keycloak -o jsonpath='{.data.admin-password}' | base64 -d)
          yq eval ".orchestrator.admin_password = \"${KC_ADMIN_PWD}\"" -i orchestrator-configs/kind.yaml
          yq eval '.infra.host.edgenode.hw_info.libvirt_pool_name = "default"' -i tests/core_foundation/data/cf_data_2_ven_VEN-libvirt_ubuntu-24.04-lts.yaml
          yq eval '.infra.host.edgenode.hw_info.libvirt_network_name = "default"' -i tests/core_foundation/data/cf_data_2_ven_VEN-libvirt_ubuntu-24.04-lts.yaml
          cat orchestrator-configs/kind.yaml
          cat tests/core_foundation/data/cf_data_2_ven_VEN-libvirt_ubuntu-24.04-lts.yaml
          source venv_edge-manage-test-automation/bin/activate
          robot -L DEBUG --pythonpath . \
            --name "VEN Onboarding and Cluster Lifecycle" \
            -d robot_output/ven_setup \
            -V orchestrator-configs/kind.yaml \
            -V tests/core_foundation/data/cf_data_2_ven_VEN-libvirt_ubuntu-24.04-lts.yaml \
            --exitonfailure \
            --include cf1 \
            --include cf3 \
            --include cf4 \
            --include cf9 \
            tests/core_foundation/core_foundation.robot
          echo "VEN onboarding, cluster creation, and cluster deletion done!"

      - name: Upload Golden Suite artifacts
        if: ${{ always() && inputs.use-ven && inputs.deploy-orchestrator }}
        uses: actions/upload-artifact@v6
        with:
          name: golden-suite-report
          path: edge-manage-test-automation/robot_output/**/*

      - name: Collect diagnostics
        if: ${{ failure() }}
        run: |
          mkdir -p diag-logs
          kubectl get pods -A -o wide > diag-logs/all-pods.log 2>&1 || true
          kubectl get deployments -A -o wide > diag-logs/all-deployments.log 2>&1 || true
          kubectl get svc -A -o wide > diag-logs/all-svcs.log 2>&1 || true
          kubectl -n orch-cluster get pods -o wide > diag-logs/orch-cluster-pods.log 2>&1 || true
          kubectl logs -n orch-cluster -l app=cluster-manager-cm -c cluster-manager --tail=-1 > diag-logs/cluster-manager.log 2>&1 || true
          kubectl logs -n orch-cluster -l app.kubernetes.io/name=cluster-connect-gateway --tail=-1 > diag-logs/cluster-connect-gateway.log 2>&1 || true
          kubectl logs -n orch-cluster -l app=intel-infra-provider-manager -c intel-infra-provider-manager --tail=-1 > diag-logs/intel-infra-provider-manager.log 2>&1 || true
          # VEN/Libvirt diagnostics
          virsh list --all > diag-logs/virsh-list.log 2>&1 || true
          virsh net-list --all > diag-logs/virsh-networks.log 2>&1 || true

      - name: Upload diagnostics
        if: ${{ failure() }}
        uses: actions/upload-artifact@v6
        with:
          name: diag-logs
          path: diag-logs/*
